{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš¢ K-Nearest Neighbors (KNN) Workshop: Titanic Implementation\n",
                "\n",
                "## ðŸ‘¨â€ðŸŽ“ Student Information\n",
                "**Name:** Ali Cihan Ozdemir  \n",
                "**Student ID:** [Insert Student ID Here]  \n",
                "\n",
                "## ðŸ“˜ Workshop Summary\n",
                "This notebook demonstrates a complete Machine Learning Pipeline using the **Titanic Dataset**.\n",
                "We implement the **Machine Learning Pipeline Pattern** to architect modular, reproducible ML code, integrating a **Remote Database** (sqlite simulation) and a **Data Stream**.\n",
                "\n",
                "### ðŸŽ¯ Objectives\n",
                "1. **Connect** to a remote database (SQLite `titanic.db`) populated by a streaming service.\n",
                "   > *Note: For a production deployment, replace the SQLite `DB_FILE` connection string with a standard `postgresql://` or `mysql://` URI pointing to an AWS RDS or Azure SQL instance. The code structure remains identical.*\n",
                "2. **Preprocess** the raw Titanic data (handle missing values, encode categorical features).\n",
                "3. **Train** a K-Nearest Neighbors (KNN) classifier to predict survival.\n",
                "4. **Visualize** the results in a Dynamic Dashboard that updates in real-time as new data arrives.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ—ï¸ 1. Database Connection & Data Loading\n",
                "\n",
                "We connect to `titanic.db`, which is being populated by our `stream_titanic.py` script."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import sqlite3\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from IPython.display import display, clear_output\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "\n",
                "# Database Configuration\n",
                "DB_FILE = 'titanic.db'\n",
                "\n",
                "def load_data_from_db():\n",
                "    \"\"\"Reads the passengers table from the SQLite database.\"\"\"\n",
                "    try:\n",
                "        conn = sqlite3.connect(DB_FILE)\n",
                "        df = pd.read_sql_query(\"SELECT * FROM passengers\", conn)\n",
                "        conn.close()\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"Error reading database: {e}\")\n",
                "        return pd.DataFrame()\n",
                "\n",
                "print(\"Checking Database connection...\")\n",
                "df_initial = load_data_from_db()\n",
                "if not df_initial.empty:\n",
                "    print(f\"Success! Use {len(df_initial)} records loaded.\")\n",
                "    display(df_initial.head())\n",
                "else:\n",
                "    print(\"Database is empty or not found. Please ensure stream_titanic.py is running.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§¹ 2. Data Preprocessing Pipeline\n",
                "\n",
                "We define a `Preprocessor` class to handle cleaning and encoding. This ensures consistency between training and streaming data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Preprocessor:\n",
                "    def __init__(self):\n",
                "        self.sex_encoder = LabelEncoder()\n",
                "        self.embarked_encoder = LabelEncoder()\n",
                "        self.scaler = StandardScaler()\n",
                "        self.imputer_age = SimpleImputer(strategy='median')\n",
                "        self.imputer_fare = SimpleImputer(strategy='median')\n",
                "        self.is_fitted = False\n",
                "\n",
                "    def fit(self, X):\n",
                "        \"\"\"Fit imputer and scaler on training data.\"\"\"\n",
                "        # Handle categorical fitting\n",
                "        # Note: In production, we'd handle unseen labels more robustly.\n",
                "        self.sex_encoder.fit(['male', 'female'])\n",
                "        self.embarked_encoder.fit(['S', 'C', 'Q', 'unknown']) # 'unknown' for NaNs\n",
                "\n",
                "        # Numerical\n",
                "        self.imputer_age.fit(X[['Age']])\n",
                "        self.imputer_fare.fit(X[['Fare']])\n",
                "        \n",
                "        # Check numerical columns after imputation for scaling fitting\n",
                "        X_temp = X.copy()\n",
                "        X_temp['Age'] = self.imputer_age.transform(X[['Age']])\n",
                "        X_temp['Fare'] = self.imputer_fare.transform(X[['Fare']])\n",
                "        self.scaler.fit(X_temp[['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']])\n",
                "        \n",
                "        self.is_fitted = True\n",
                "\n",
                "    def transform(self, X):\n",
                "        \"\"\"Transform data using fitted parameters.\"\"\"\n",
                "        X = X.copy()\n",
                "        \n",
                "        # Drop irrelevant features\n",
                "        drop_cols = ['Name', 'Ticket', 'Cabin', 'PassengerId']\n",
                "        X = X.drop(columns=[c for c in drop_cols if c in X.columns])\n",
                "        \n",
                "        # Encode Sex\n",
                "        X['Sex'] = X['Sex'].map(lambda s: 1 if s == 'female' else 0) # Manual safer here\n",
                "\n",
                "        # Encode Embarked\n",
                "        X['Embarked'] = X['Embarked'].fillna('S') # Default to S\n",
                "        X['Embarked'] = X['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
                "\n",
                "        # Impute\n",
                "        X['Age'] = self.imputer_age.transform(X[['Age']])\n",
                "        X['Fare'] = self.imputer_fare.transform(X[['Fare']])\n",
                "        \n",
                "        # Scale\n",
                "        X_scaled = self.scaler.transform(X[['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']])\n",
                "        \n",
                "        # Combine\n",
                "        X_final = np.column_stack((X_scaled, X['Sex'], X['Embarked']))\n",
                "        return X_final\n",
                "\n",
                "print(\"Preprocessor class defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ¤– 3. Model Training (KNN)\n",
                "\n",
                "We will initially train the model on the data currently available in the database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load current data\n",
                "df = load_data_from_db()\n",
                "\n",
                "if len(df) > 10:\n",
                "    # Preprocessing\n",
                "    preprocessor = Preprocessor()\n",
                "    X = df.drop(columns=['Survived'])\n",
                "    y = df['Survived']\n",
                "    \n",
                "    preprocessor.fit(X)\n",
                "    X_processed = preprocessor.transform(X)\n",
                "    \n",
                "    # Train/Test Split\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
                "    \n",
                "    # Initialize KNN\n",
                "    knn = KNeighborsClassifier(n_neighbors=5)\n",
                "    knn.fit(X_train, y_train)\n",
                "    \n",
                "    # Evaluate\n",
                "    y_pred = knn.predict(X_test)\n",
                "    print(f\"Initial Model Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
                "    print(classification_report(y_test, y_pred))\n",
                "else:\n",
                "    print(\"Not enough data to train yet. Waiting for stream...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š 4. Dynamic Dashboard\n",
                "\n",
                "This section implements the real-time visualization. It polls the database every few seconds, processes new data, updates the counts, and visualizes predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dynamic Dashboard Settings\n",
                "UPDATE_INTERVAL = 3 # seconds\n",
                "MAX_ITERATIONS = 20 # Run for a limited time for demo\n",
                "\n",
                "try:\n",
                "    for i in range(MAX_ITERATIONS):\n",
                "        # 1. Fetch updated data\n",
                "        df_live = load_data_from_db()\n",
                "        \n",
                "        if df_live.empty:\n",
                "            time.sleep(UPDATE_INTERVAL)\n",
                "            continue\n",
                "            \n",
                "        # 2. Process Data\n",
                "        if not preprocessor.is_fitted:\n",
                "             # Fit on initial batch if not already done\n",
                "            X_live = df_live.drop(columns=['Survived'])\n",
                "            preprocessor.fit(X_live)\n",
                "            \n",
                "        X_live_processed = preprocessor.transform(df_live.drop(columns=['Survived']))\n",
                "        y_live = df_live['Survived']\n",
                "        \n",
                "        # 3. Retrain/Update Model (Optional, or just predict)\n",
                "        # For this workshop, we'll retrain on all available data to show 'learning'\n",
                "        knn.fit(X_live_processed, y_live)\n",
                "        \n",
                "        # 4. Visualization\n",
                "        clear_output(wait=True)\n",
                "        plt.figure(figsize=(15, 5))\n",
                "        \n",
                "        # Plot 1: Survival Count\n",
                "        plt.subplot(1, 3, 1)\n",
                "        sns.countplot(x='Survived', data=df_live, palette='viridis')\n",
                "        plt.title(f'Live Survival Counts (Total: {len(df_live)})')\n",
                "        plt.xlabel('Survived (0=No, 1=Yes)')\n",
                "        \n",
                "        # Plot 2: Class Distribution\n",
                "        plt.subplot(1, 3, 2)\n",
                "        sns.countplot(x='Pclass', hue='Survived', data=df_live, palette='coolwarm')\n",
                "        plt.title('Survival by Passenger Class')\n",
                "        \n",
                "        # Plot 3: Fare Distribution\n",
                "        plt.subplot(1, 3, 3)\n",
                "        sns.histplot(df_live['Fare'], bins=20, kde=True, color='purple')\n",
                "        plt.title('Live Fare Distribution')\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "        \n",
                "        print(f\"Iteration {i+1}/{MAX_ITERATIONS} - Last updated: {time.strftime('%H:%M:%S')}\")\n",
                "        print(f\"Total Records: {len(df_live)}\")\n",
                "        \n",
                "        time.sleep(UPDATE_INTERVAL)\n",
                "        \n",
                "except KeyboardInterrupt:\n",
                "    print(\"Dashboard stopped by user.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ’­ 5. Reflection\n",
                "\n",
                "### Performance\n",
                "The KNN model typically achieves around 70-80% accuracy on the Titanic dataset. The performance improves as more data is streamed in, capturing more variance in the `Fare` and `Age` distributions.\n",
                "\n",
                "### Preprocessing Decisions\n",
                "- **Imputation**: Median imputation was used for `Age` and `Fare` to handle outliers better than mean.\n",
                "- **Feature Selection**: `Name`, `Ticket`, and `Cabin` were dropped to simplify the model, though `Cabin` could have been engineered into Deck levels.\n",
                "- **Encoding**: `Sex` and `Embarked` were encoded numerically as KNN requires numeric distance calculations.\n",
                "\n",
                "### Strengths and Weaknesses of KNN\n",
                "- **Strengths**: Simple to understand, instance-based (adapts immediately to new data without explicit training phase, though sklearn implementations do refit).\n",
                "- **Weaknesses**: Computationally expensive at prediction time as data grows (O(n)), sensitive to outliers and feature scaling.\n",
                "\n",
                "### MLOps Extension\n",
                "In a production environment, this pipeline could be automated using Airflow or Kubeflow. The \"Database\" would be a cloud warehouse (Snowflake/BigQuery), and the Dashboard could be served via Streamlit or Tableau reading from that source."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}